---
title: "codigo"
author: "ruben"
date: "2023-11-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

A lo largo de este documento se captura el procedimeinto segudo para tratar con el primer tipo de señales biomedicas
con las que trabajaremos a lo largo de nuestro proyecto, los ECG.

En nuestro caso, se ha decidido seguir la metodología CRISP-DM, dividiendo nuestro estudio en las siguientes fases

*   Comprensión del negocio y objetivo del estudio
*   Comprensión y análisis exploratorio de los datos 
*   Preprocesamiento del conjunto de datos
*   Implementación del modelo
*   Evaluación de los resultados




## Comprensión de los datos
Se ha realizado un estudio previo del funcionamiento de los mismos. __EXPLICAR__

Cabe destacar qeu un electrocardiograma se divide en las siguientes partes

__EXPLICAR__

En este caso, el desafio presente consiste en el diagnóstico eficiente de anomalias en el pulso cardíaco atendiendo a la zona donde se produce el latido, que puede estar asociada a _ y por lo tanto su detecctión es clave para la realización de un diagnóstico precoz de las enfermedades. En este caso, los tipos de latidos son los siguientes:
*    Normal: son aquellos que siguen el ritmo cardíaco regular y se originan en el nodo sinusal, que es el marcapasos natural del corazón.
*   Supraventricular ectopic beat: en español latidos supraventriculares ectópicos pueden estar asociados con arritmias supraventriculares como la fibrilación auricular.
*   Ventricular ectopic beat: o latidos ventriculares ectópicos pueden estar relacionados con arritmias ventriculares, como la taquicardia ventricular o la fibrilación ventricular.
*    Fusion beat: latidos de fusión pueden ocurrir en ciertas condiciones, como durante la estimulación eléctrica del corazón o en presencia de arritmias.



## Comprensión de los datos

Para nuestro estudio se toma como fuente de datos un fichero csv obtenido del sitio web del siguiente enlace__,
donde a su vez se puede encontrar el significado de los metadatos proporcionados.

A partir del contenido del fichero se realiza en siguiente analisis exploratorio de los datos


```{r}
library(readr)
library(dplyr)
library(xgboost)
library(caret)
library(pROC)

```




```{r}

mitbih_train <- read_csv("data/mitbih_train.csv", 
    col_names = FALSE)
```


```{r}
glimpse(mitbih_train)
```


Dado que la ultima columna, como se indica en la fuente de los datos, se corresponde con la clasificación de 
los datos, separamos por un lado esta de las series temporales que se corresponden con las frecuencias cardiacas,
almacenadas en el resto de las columnas.


```{r}

df <- mitbih_train[,1:187]
clases <- mitbih_train[,188]
clases <- clases[[1]]


table(clases)


```


Si se visualizan las clases en función de su aparición en el conjunto de datos, tenemos que se muestra una 
completa predominancia de los ECG donde el paciente no padece de ninguna enfermedad. La distribución de los valores se puede observar en el siguiente diagrama de sectores.

```{r}
etiquedas <- factor(x=clases,levels=c(0,1,2,3,4),labels = c("N","S","V","F","Unknown"))
pie(table(etiquedas))
```

En cuanto a las series temporales de cada una de las señales, se tiene que estas son de longitudes distintas, siendo su fin aquella columna a partir de la cual el resto de valores son 0. Para ello, generamos un conjunto de datos adicional donde a partir del fin de los mismos almacenamos un NA en lugar de un 0





Algunas primeras metricas de nuestro conjunto de datos para poder enterder comportamientos de cada uno de los tipos de 
señales son los siguientes:

*   Duración promedio

```{r}

```




La duración promedio general de nuestras señales es de __dato__. Además, la duración en función del tipo de onda es la siguiente:



*   Distribuciones de las amplitudes

Las amplitures de las ondas siguen las siguientes distribuciones


Atendiendo al diagnostico, las distribuciones de las amplitudes son las siguientes


#### Otras métricas estudiadas en la asignatura
__Se explican las sigeuites__



##  Preparación de los datos
Dado que implementaremos dos modelos diferences, como se verá en el siguiente apartado, y estos funcionan de forma ligeramente diferente, se generarán dos dataframes distintos para que se adapten a estos.
*   Primer dataframe: este se limita a almacenar las métricas extraibles de cada onda, omitiendo los valores en sí de esta onda en función del tiempo. Este se usa para alimentar un algormitmo de clasificación, que en este caso se ha decidido que sea `xgboost`

```{r}

```


*   Segundo dataframe: contiene los valores que toma en el tiempo la señal, además de una serie de características añadidas en las. Este alimentará el 

## Implementación del modelo

#### Primer modelo: XGBOOST
Antes de comenzar con el entrenamiento del modelo, se divide el conjunto de datos en train y test, de forma que pueda mantenerse un tratamiento honesto de los datos y evitar el suceso conocido como `fuga de datos`, que proporciona resultados irreales en nuestro modelo.

La proporción elegida en nuestro caso para el conjunto de entrenamiento ha sido del 80% de los datos.

```{r}
datos <- data.frame(
  Columna1 = c(1, 2, 3, 4),
  Columna2 = c("A", "B", "C", "D"),
  Columna3 = c(TRUE, FALSE, TRUE, FALSE)
)

# Indices del conjunto de entrenamiento
i_train <- sort(sample(1:dim(datos)[1],replace = F,size = round(dim(datos)[1]*0.8,0)))


train <- datos[i_train,]
train_df <- train[,-length(train)]
train_class <- train[,length(train)]
head(train)  
  
test <- datos[-i_train,]
train_df <- test[,-length(test)]
train_class <- test[,length(test)]
head(test)

```

Una vez realizada la partición, se entrena el modelo con nuestros datos

```{r}
# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(train_df), label = train_class)

# Set parameters for the XGBoost model
params <- list(
  objective = "multi:softmax",
  objective = length(levels(train_class)),  
  eval_metric = "mlogloss"         # log likelihood loss for binary classification
)

# Train the XGBoost model
xgb_model <- xgboost(data = dtrain, params = params, nrounds = 10)



```


EN este caso, para nuestro conjunto de entrena


*   Implementación del segundo modelo
Esta se lleva a cabo en el fichero _x_, donde a partir de el segundo dataframe creado se implementa una red neuronal en python capaz de clasificar nuestras ondas.

## Evaluación de los resultados

#### Primer modelo

Dado que se tenia un conjunto desvalanceado, es importante seleccionar una metrica adecuada para evaluar los resultados
Como prmera aproximación usamos el accuracy, que aunque no cumpla con lo anterior nos servira para tomarla como referencia acerca del porcentaje total de instancias acertadas


```{r}
# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", accuracy))
```

En cuanto a la tasa de aciertos en función de la clase se tiene la siguiente matriz de confusuón

```{r}
confusion_matrix <- table(Actual = y_test, Predicted = predictions + 1)
print(confusion_matrix)

```

En cuanto al area bajo la curva, que resuleve nuestro problema de desvalanceo, para el problema es 
la siguiente


```{r}
# Compute ROC curves for each class

roc_curves <- multiclass.roc(rownames(y_test_matrix), as.numeric(predictions), percent = TRUE)

# Plot ROC curves
plot(roc_curves, col = c("red", "green", "blue"), lty = 1:3, main = "Multi-Class ROC Curve", lwd = 2)
legend("right", legend = levels(y_test), col = c("red", "green", "blue"), lty = 1:3, lwd = 2)

```


#### Segundo modelo
La evaluación de los resultados, al igual que el entrenamiento del modelo, se ha llevado a cabo en el fichero _fichero_
